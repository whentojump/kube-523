# Alarms

## Alarm1-2

Test info: trial-02-0012/0001, trial-06-0010/0001

### What happened
Field spec.tidb.requests.ACTOKEY: Invalid input detected and Found no matching fields for input.
Field spec.tidb.slowLogTailer.requests.ACTOKEY: Invalid input detected and Found no matching fields for input. prev='NotPresent', curr='3E+.690'

### Categorization
False Alarm

### **Root Cause**
The error message in operator.log shows: 
```json
Requests: unmarshalerDecoder: quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', error found in #10 byte of ...|934403.35"},"service|..., bigger context ...|eplicas":1,"requests":{"ACTOKEY":"80e+81934403.35"},"service":{"type":"ClusterIP"}},"tikv":{"baseIma|...
```
Because the ACTOKEY does not meet with the correct regular expression, cluster reject to change with the new input and it causes the false alarm.


## Alarm3-4

Test info: trial-02-0000/0001, trial-04-0012/0001

### What happened
Field spec.tidb.limits.ACTOKEY: Invalid input detected and Found no matching fields for input.
Field spec.tidb.slowLogTailer.limits.ACTOKEY: Invalid input detected


### Categorization
False Alarm

### **Root Cause**
The error message in operator.log shows: 
```json
Limits: unmarshalerDecoder: quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', error found in #10 byte of ...|88E-.5400"},"maxFail|..., bigger context ...|config":{},"limits":{"ACTOKEY":"9491.67088E-.5400"},"maxFailoverCount":0,"replicas":1,"service":{"ty|
```
Because the ACTOKEY does not meet with the correct regular expression, cluster reject to change with the new input and it causes the false alarm.


## Alarm5-8

Test info: trial-04-0006/0002, trial-01-0013/0004, trial-00-0000/0006, trial-00-0000/0008

### What happened
Acto is trying to modify several fields of initContainers, including ['spec']['tidb']['initContainers'][0][env][0][valueFrom][resourceFieldRef][divisor].
Acto is trying to modify ['tidb']['env'][3][valueFrom][resourceFieldRef][divisor] from "NotPresent" to "curr": "5265144518.E+.00262103".

### Categorization
False Alarm

### **Root Cause**
The error message in operator.log shows: 
```json
Divisor: unmarshalerDecoder: quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', error found in #10 byte of ...|.93760244","resource|..., bigger context ...|ontainerName":"ACTOKEY","divisor":"+8.6e.93760244","resource":"ACTOKEY"},"secretKeyRef":{"key":"ACTO|...
```
Because the ACTOKEY does not meet with the correct regular expression, cluster reject to change with the new input and it causes the false alarm.

## Alarm 9

Test info: trial-06-0002/010

### What happened
Acto creates spec.tidb.imagePullSecrets from "" to "ACTOKEY" correctly, but when deletes ACTOKEY, the system state is not changed.

### Categorization
True Alarm

### **Root Cause**
same as alarm 7

### Expected Behavior
Add tidb.additionalContainers.image field and requests field.


## Alarm 10-16

Test info: trial-05-0002/0001, trial-00-0000/0001, trial-00-0019/0007, trial-03-0002/0003, trial-05-0002/0001, trial-06-0012/0001, trial-07-0018/0002

### What happened
Acto try to set the following test:
```json
    initContainers:
    - name: ceerfnmalj
      readinessProbe:
        httpGet:
          path: /invalid-path
```
But cli-output.log shows the following error:
```log
error: error validating \"testrun-2024-03-06-11-23/trial-05-0002/mutated-001.yaml\": error validating data: ValidationError(TidbCluster.spec.tidb.initContainers[0].readinessProbe.httpGet): missing required field \"port\" in com.pingcap.v1alpha1.TidbCluster.spec.tidb.initContainers.readinessProbe.httpGet; if you choose to ignore these errors, turn validation off with --validate=false"
```
### Categorization
False Alarm

### **Root Cause**
The validation program in operator will check readinessProbe.httpGet.port field. If it doesn't exist, this update will not be finished.

### Expected Behavior
Add readinessProbe.httpGet.port field to the test of Acto and do the test again.



## Alarm 17

Test info: trial-01-0001/0001

### What happened
Acto try to set the following test:
```json
    topologySpreadConstraints:
    - {}
```
But cli-output.log shows the following error:
```log
error: error validating \"testrun-2024-03-06-11-23/trial-01-0001/mutated-001.yaml\": error validating data: ValidationError(TidbCluster.spec.tidb.topologySpreadConstraints[0]): missing required field \"topologyKey\" in com.pingcap.v1alpha1.TidbCluster.spec.tidb.topologySpreadConstraints; if you choose to ignore these errors, turn validation off with --validate=false"
```
### Categorization
False Alarm

### **Root Cause**
The validation program in operator will check topologyKey field. If it doesn't exist, this update will not be finished.

### Expected Behavior
Add topologySpreadConstraints.topologyKey field to the test of Acto and do the test again.


## Alarm 18

Test info: /trial-01-0011/0008

### What happened
Acto try to set the following test:
```json
    additionalVolumes:
    - ephemeral:
        volumeClaimTemplate: {}
      name: ubbuokrmbb
```
But cli-output.log shows the following error:
```log
error: error validating \"testrun-2024-03-06-11-23/trial-01-0011/mutated-008.yaml\": error validating data: ValidationError(TidbCluster.spec.tidb.additionalVolumes[0].ephemeral.volumeClaimTemplate): missing required field \"spec\" in com.pingcap.v1alpha1.TidbCluster.spec.tidb.additionalVolumes.ephemeral.volumeClaimTemplate; if you choose to ignore these errors, turn validation off with --validate=false

```
### Categorization
False Alarm

### **Root Cause**
same as alarm 10

### Expected Behavior
same as alarm 10

## Alarm 19-20

Test info: trial-03-0009/0009, trial-03-0010/0001

### What happened
Acto try to set the following test:
```json
    tidb: {}
```
But cli-output.log shows the following error:
```log
error: error validating \"testrun-2024-03-06-11-23/trial-03-0009/mutated-009.yaml\": error validating data: ValidationError(TidbCluster.spec.tidb): missing required field \"replicas\" in com.pingcap.v1alpha1.TidbCluster.spec.tidb; if you choose to ignore these errors, turn validation off with --validate=false

```
### Categorization
False Alarm

### **Root Cause**
same as alarm 10

### Expected Behavior
same as alarm 10


## Alarm 21-23

Test info: trial-03-0011/0001, trial-04-0005/0003,trial-04-0005/0005 

### What happened
Acto try to set the following test:
```json
    additionalVolumes:
    - {}
```
But cli-output.log shows the following error:
```log
error: error validating \"testrun-2024-03-06-11-23/trial-03-0011/mutated-001.yaml\": error validating data: ValidationError(TidbCluster.spec.tidb.additionalVolumes[0]): missing required field \"name\" in com.pingcap.v1alpha1.TidbCluster.spec.tidb.additionalVolumes; if you choose to ignore these errors, turn validation off with --validate=false

```
### Categorization
False Alarm

### **Root Cause**
same as alarm 10

### Expected Behavior
same as alarm 10


## Alarm 24 - 31

Test info: trial-00-0008/001, trial-07-0006/0002, trial-07-0006/0003, trial-07-0007/0001, trial-00-0006/0002, trial-00-0006/0004, trial-00-0007/0001, trial-00-0008/0001

### What happened
Acto changed tidb.additionalContainers.requests.ACTOKEY from "" to 1000m, but system state is not changed.

### Categorization
False Alarm

### **Root Cause**
In the source code of adding additional Containers:
```go
	podSpec.Containers, err = MergePatchContainers(containers, baseTiDBSpec.AdditionalContainers())
	if err != nil {
		return nil, fmt.Errorf("failed to merge containers spec for TiDB of [%s/%s], error: %v", ns, tcName, err)
	}
	podSpec.InitContainers = append(initContainers, baseTiDBSpec.InitContainers()...)
```
Inside MergePatchContainers function, it merges the containers with the AdditionalContainers. The root cause should be errors on merge containers when there are no initContainers in the system.

### Expected Behavior
Change the acto's test case by adding initContainers and additional containers. Otherwise, add validation function in the operator which can check initContainers.


## Alarm 32-40

Test info: trial-00-0002/001, trial-00-0001/0001, trial-00-0003/0001, trial-00-0004/0001, trial-03-0000/0005, trial-03-0001/0001, trial-03-0011/0003, trial-03-0012/0001, trial-07-0011/0001

### What happened
Acto changed tidb.slowLogTailer.limits.ACTOKEY from "" to 2, but system state is not changed.

### Categorization
True Alarm

### **Root Cause**
In the operator's souce code:

```go
containers = append(containers, corev1.Container{
		Name:            v1alpha1.ContainerSlowLogTailer.String(),
		Image:           tc.HelperImage(),
		ImagePullPolicy: tc.HelperImagePullPolicy(),
		Resources:       controller.ContainerResource(tc.Spec.TiDB.GetSlowLogTailerSpec().ResourceRequirements),
		VolumeMounts:    []corev1.VolumeMount{slowQueryLogVolumeMount},
		Command: []string{
			"sh",
			"-c",
			fmt.Sprintf("touch %s; tail -n0 -F %s;", slowLogFileEnvVal, slowLogFileEnvVal),
		},
	})

func ContainerResource(req corev1.ResourceRequirements) corev1.ResourceRequirements {
	trimmed := req.DeepCopy()
	if trimmed.Limits != nil {
		delete(trimmed.Limits, corev1.ResourceStorage)
	}
	if trimmed.Requests != nil {
		delete(trimmed.Requests, corev1.ResourceStorage)
	}
	return *trimmed
}

```
The slowlogTailerspec's resource will be deleted by containerResource function, which causes the limit field's value cannot be updated.

### Expected Behavior
In operator's "if tc.Spec.TiDB.ShouldSeparateSlowLog()" code module, add spec.tidb.slowLogTailer.limit and spec.tidb.slowLogTailer.request field to tc.


## Alarm 41-51

Test info: /trial-01-0002/0002, trial-01-0009/0005, trial-01-0015/0004, trial-02-0015/0006, trial-04-0006/0006, trial-05-0004/0002, trial-05-0004/0003, trial-05-0005/0001, trial-05-0005/0002, trial-05-0005/0003, trial-05-0006/0001

### What happened
Acto changed tidb.EnvFrom from ACTOKEY to not present, but system state is not changed.

### Categorization
True Alarm

### **Root Cause**
In the operator's souce code:

```go
	c := corev1.Container{
		Name:            v1alpha1.TiDBMemberType.String(),
		Image:           tc.TiDBImage(),
		Command:         []string{"/bin/sh", "/usr/local/bin/tidb_start_script.sh"},
		ImagePullPolicy: baseTiDBSpec.ImagePullPolicy(),
		Ports: []corev1.ContainerPort{
			{
				Name:          "server",
				ContainerPort: v1alpha1.DefaultTiDBServerPort,
				Protocol:      corev1.ProtocolTCP,
			},
			{
				Name:          "status", // pprof, status, metrics
				ContainerPort: v1alpha1.DefaultTiDBStatusPort,
				Protocol:      corev1.ProtocolTCP,
			},
		},
		VolumeMounts: volMounts,
		Resources:    controller.ContainerResource(tc.Spec.TiDB.ResourceRequirements),
		Env:          util.AppendEnv(envs, baseTiDBSpec.Env()),
		EnvFrom:      baseTiDBSpec.EnvFrom(),
		ReadinessProbe: &corev1.Probe{
			ProbeHandler:        buildTiDBReadinessProbHandler(tc),
			InitialDelaySeconds: int32(10),
		},
	}

```
The EnvFrom's resource will be updated everytime the new baseTIDBSpec is applied. But I cannot see the root reason that causes this field not updated.

### Expected Behavior
Modify the logic of changing EnvFrom update logic.



## Alarm 52-53

Test info: trial-00-0011/0007, trial-00-0012/0001

### What happened
Acto changed
```json    
additionalVolumes:
    - downwardAPI:
        items:
        - fieldRef:
            apiVersion: v2
            fieldPath: nxyzdmgbwz
          path: utkmttwzws
      ephemeral:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                ACTOKEY: 2000m
```
  from not present to 2000m, but system state is not changed.

### **Root Cause**
same as Alarm 24

### Expected Behavior
same as Alarm 24